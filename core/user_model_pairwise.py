# -*- coding: utf-8 -*-
# @Time    : 2021/7/27 3:31 下午
# @Author  : Chongming GAO
# @FileName: user_model_pairwise.py

import torch
from deepctr_torch.inputs import combined_dnn_input, build_input_features
from deepctr_torch.layers import DNN, PredictionLayer, FM
from torch import nn

from core.layers import Linear
from core.user_model import UserModel, compute_input_dim, create_embedding_matrix


class UserModel_Pairwise(UserModel):
    """Instantiates the Multi-gate Mixture-of-Experts architecture.

    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.
    :param tasks: list of str, indicating the loss of each tasks, ``"binary"`` for  binary logloss, ``"regression"`` for regression loss. e.g. ['binary', 'regression']
    :param num_experts: integer, number of experts.
    :param expert_dim: integer, the hidden units of each expert.
    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of shared-bottom DNN
    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector
    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN
    :param init_std: float,to use as the initialize std of embedding vector
    :param task_dnn_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN
    :param seed: integer ,to use as random seed.
    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.
    :param dnn_activation: Activation function to use in DNN
    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN
    :param device: str, ``"cpu"`` or ``"cuda:0"``

    :return: A PyTorch model instance.
    """

    def __init__(self, feature_columns, y_columns, task, task_logit_dim,
                 dnn_hidden_units=(128, 128),
                 l2_reg_embedding=1e-5, l2_reg_dnn=1e-1, init_std=0.0001, task_dnn_units=None, seed=2022, dnn_dropout=0,
                 dnn_activation='relu', dnn_use_bn=False, device='cpu', padding_idx=None, ab_columns=None):

        super(UserModel_Pairwise, self).__init__(feature_columns, y_columns,
                                             l2_reg_embedding=l2_reg_embedding,
                                             init_std=init_std, seed=seed, device=device,
                                             padding_idx=padding_idx)

        self.feature_columns = feature_columns
        self.feature_index = self.feature_index

        self.y_columns = y_columns
        self.task_logit_dim = task_logit_dim

        self.sigmoid = nn.Sigmoid()
        """
        For MMOE Layer
        """
        self.task = task
        self.task_dnn_units = task_dnn_units


        """
        For DNN Layer.
        """

        self.dnn = DNN(compute_input_dim(self.feature_columns), dnn_hidden_units,
                            activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,
                            init_std=init_std, device=device)
        self.last = nn.Linear(dnn_hidden_units[-1], 1, bias=False)
        self.out = PredictionLayer(task, 1)


        """
        For FM Layer.
        """
        use_fm = True if task_logit_dim == 1 else False
        self.use_fm = use_fm

        self.fm_task = FM() if use_fm else None

        self.linear = Linear(self.feature_columns, self.feature_index, device=device)

        """
        For exposure effect
        """
        if ab_columns is not None:
            ab_embedding_dict = create_embedding_matrix(ab_columns, init_std, sparse=False, device=device)
            for tensor in ab_embedding_dict.values():
                nn.init.normal_(tensor.weight, mean=1, std=init_std)

            self.ab_embedding_dict = ab_embedding_dict

        self.ab_columns = ab_columns

        self.add_regularization_weight(self.parameters(), l2=l2_reg_dnn)

        self.to(device)


    def _deepfm(self, X, feature_columns, feature_index):

        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, feature_columns,
                                                                                  self.embedding_dict,
                                                                                  feature_index=feature_index)
        dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)


        linear_model = self.linear
        dnn = self.dnn
        last = self.last
        out = self.out

        # Linear and FM logit
        logit = torch.zeros([len(X), self.task_logit_dim], device=X.device)

        if linear_model is not None:
            logit = logit + linear_model(X)

            fm_model = self.fm_task
            if self.use_fm and len(sparse_embedding_list) > 0 and fm_model is not None:
                fm_input = torch.cat(sparse_embedding_list, dim=1)
                logit += fm_model(fm_input)

        linear_logit = logit

        # DNN
        dnn_logit = out(last(dnn(dnn_input)))

        y_pred = linear_logit + dnn_logit

        return y_pred

    def get_loss(self, x, y, score):

        X_pos = x[:, :7]
        X_neg = x[:, 7:]


        # y_deepfm_pos = self._deepfm(X_pos, self.feature_columns, self.feature_index)
        # y_deepfm_neg = self._deepfm(X_neg, self.feature_columns, self.feature_index)
        y_deepfm_pos = self.forward(X_pos)
        y_deepfm_neg = self.forward(X_neg)


        if self.ab_columns is None:
            loss = self.loss_func(y, y_deepfm_pos, y_deepfm_neg, score)
        else:  # CIRS-UserModel-kuaishou.py
            alpha_u = self.ab_embedding_dict['alpha_u'](x[:,0].long())
            beta_i = self.ab_embedding_dict['beta_i'](x[:,1].long())
            loss = self.loss_func(y, y_deepfm_pos, y_deepfm_neg, score, alpha_u=alpha_u, beta_i=beta_i)

        return loss

    def forward(self, x):
        y_deepfm = self._deepfm(x, self.feature_columns, self.feature_index)
        return y_deepfm
